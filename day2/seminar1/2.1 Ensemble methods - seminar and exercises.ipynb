{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembles\n",
    "Lecture recap\n",
    "1. Combine many weak algorithms into one strong\n",
    "2. Most of them use decision trees as base estimators - they are fast, invariant to scaling, and handle features of different nature well\n",
    "3. For problems like Higgs (small number of features of different origin) ensembles of decision trees is state-of-the-art.\n",
    "\n",
    "\n",
    "Let's try them out on simple synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x_space = np.random.uniform(0, 1, size=60)\n",
    "y0 = x_space + 0.2 + np.random.normal(size=len(x_space), scale=0.1)\n",
    "y1 = x_space + np.random.normal(size=len(x_space), scale=0.1)\n",
    "shuffle = np.random.permutation(2*len(x_space))\n",
    "\n",
    "\n",
    "X = np.concatenate((np.concatenate((x_space, x_space))[:, np.newaxis],\n",
    "                          np.concatenate((y0, y1))[:, np.newaxis]), axis=1)[shuffle]\n",
    "\n",
    "y = np.concatenate((np.zeros(len(y0)), np.ones(len(y1))))[shuffle]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='none', s=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X = scale(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## RandomForest\n",
    "1. Uses bagging + random subspace method on Decision Trees\n",
    "2. Isn't prone to overfitting\n",
    "3. Easily parallelized\n",
    "\n",
    "Let's see how decision surface for ReandomForest changes with incerase in the number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "n_classes = 2\n",
    "plot_colors = \"bry\"\n",
    "plot_step = 0.02\n",
    "\n",
    "def plot_decision_surface(clf, X, y):\n",
    "    \"\"\"Draws colored rectangles representing decision rule\"\"\"\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.axis(\"tight\")\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "plot_decision_surface(LogisticRegression().fit(X,y), X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimators_set = (1, 2, 3, 4, 5, 10, 20, 30, 50, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot decision surface of random forest with different number of estimators\n",
    "<Your code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Boosting\n",
    "1. Ensemble members are trained consequently to compensate for imperfection\n",
    "2. Will overfit, especially if strong base algorithms are used\n",
    "3. Can be parallelized (Google is your friend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "tracks_dataset = \"../../DS_1_train.csv\"\n",
    "electrons_dataset = \"../../DS_1_electron_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['X', 'Y', 'Z', 'TX', 'TY', 'chi2']\n",
    "tracks = pd.read_csv(tracks_dataset, index_col=\"index\", usecols=[\n",
    "    'index'] + features + ['signal'])\n",
    "train, test = train_test_split(tracks, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train[features]\n",
    "X_test = test[features]\n",
    "Y_train = train['signal']\n",
    "Y_test = train['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(n_jobs=-1, n_estimators=20).fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(test.signal, xgb.predict_proba()[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(list(range(len(xgb.feature_importances_))), xgb.feature_importances_)\n",
    "ax.set_xticklabels([\"\"] + features)\n",
    "ax.set_ylabel(\"Feature importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ensembles\n",
    "\n",
    "[A classic Kaggle technique](https://mlwave.com/kaggle-ensembling-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1 - simple voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [xgboost.XGBClassifier(n_jobs=-1, n_estimators=20, random_state=4325),\n",
    "          xgboost.XGBClassifier(n_jobs=-1, n_estimators=20, random_state=422),\n",
    "          RandomForestClassifier(n_jobs=-1, n_estimators=20, random_state=35252),\n",
    "          Ridge(random_state=141),\n",
    "          LogisticRegression(n_jobs=-1, random_state=21459)]\n",
    "\n",
    "\n",
    "\n",
    "#... something wrong with one of the models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<fit all the models>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    \n",
    "    print(\"Model = \",model)\n",
    "    print(\"AUC = \",roc_auc_score(test.signal, xgb.predict_proba()[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(models, X):\n",
    "    \"\"\"\n",
    "    Voting classifier that works by averaging predicted probabilities among models.\n",
    "    Reminder: you can use model.predict_proba(X) to get probabilities for each class\n",
    "    \"\"\"\n",
    "\n",
    "    <implement prediction by voting among models>\n",
    "    \n",
    "    return <your code: array of averaged probabilities>\n",
    "    \n",
    "prediction = predict(models,X_test)\n",
    "\n",
    "assert prediction.ndim==2 and prediction.shape[1]==2, \"Predicted probabilities must be a matrix[n,2]\"\n",
    "assert np.allclose(prediction.sum(axis=-1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(Y_test, predict(models,X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1.5 - simple voting of less correlated algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(test.signal, np.average(predictions[1:], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Level - weighting proportional to model quality\n",
    "\n",
    "You can also average your models with unequal weights: better models get more weights, worse models are less significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = <compute ROC AUC score for each model on validation>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_weighted(models, X, weights):\n",
    "    \"\"\"\n",
    "    Voting classifier that works by averaging predicted probabilities among models.\n",
    "    Note: you can use np.average(...,weights=weights) to get weighted average (or do it manually)\n",
    "    \"\"\"\n",
    "\n",
    "    <implement prediction by voting among models>\n",
    "    \n",
    "    return <your code: array of averaged probabilities>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Even better\n",
    "roc_auc_score(Y_test,predict_weighted(models,X_test,weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bonus bonus: try using different weights. For example, downweight logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2 - stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_level_one, train_level_two = train_test_split(train, random_state=517)\n",
    "\n",
    "X_train_one = train_level_one[features]\n",
    "X_train_two = train_level_two[features]\n",
    "Y_train_one = train_level_one['signal']\n",
    "Y_train_two = train_level_two['signal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [xgboost.XGBClassifier(n_jobs=-1, n_estimators=100, random_state=422),\n",
    "          RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=35252),\n",
    "          #<any other model you want>\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<fit each model>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "level_two_predictions = np.array([model.predict_proba(X_train_one) for model in models]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_model = xgboost.XGBClassifier(n_jobs=-1).fit(level_two_predictions, Y_train_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "level_two_predictions_test = np.array([model.predict_proba(X_train_one) for model in models]).T\n",
    "final_predictions_test = stacked_model.predict_proba(level_two_predictions_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Plot learning curve for XGBoost. On X axis there should be the training dataset size, on Y axis - cross-validation performance.\n",
    "\n",
    "Learning curve is useful when selecting a dataset size for experimentation as execution time grows with the dataset size.\n",
    "1. http://scikit-learn.org/stable/modules/generated/sklearn.learning_curve.validation_curve.html#sklearn.learning_curve.validation_curve\n",
    "2. Bonus points for plotting errors, for example with http://matplotlib.org/api/axes_api.html#matplotlib.axes.Axes.fill_between"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** Check whether feature importance as reported makes sense. Try removing each feature, plot the quality on cross-validation as a function of the feature removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise $\\infty$** Win the Kaggle! A few suggestions to try:\n",
    " 1. Experiment to find the best parameters (see next seminar for the advanced techniques)\n",
    " 2. Use multiple classifiers (possibly on different features) and average the predictions\n",
    " 3. Use stacking"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
