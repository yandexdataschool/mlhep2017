{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DImensionality reduction and manifold learning (embeddings)\n",
    "* Today we are going to learn how to visualize and explore the data\n",
    "![pictcha](http://sarahannelawless.com/wp-content/uploads/2015/03/tw-1-600x449.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load digits\n",
    " * A thousand something 8x8 handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a few testimonials\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X[0].reshape(8,8))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X[1].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Here's an overly complicated function that draws any two-dimensional space you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import offsetbox\n",
    "def plot_embedding(X,y=None,ax=None,show_images=True,min_dist=5e-3,figsize=[12,10]):\n",
    "    \n",
    "    #normalize\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=figsize)\n",
    "        ax = plt.subplot(1,1,1)\n",
    "\n",
    "    # scatter-plot\n",
    "    if y is None:\n",
    "        plt.scatter(*X.T)\n",
    "    else:\n",
    "        assert y is not None\n",
    "        #рисуем циферки a-la scatter\n",
    "        for i in range(X.shape[0]):\n",
    "            ax.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                     color= plt.cm.Set1(y[i] / 10.),\n",
    "                     fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    if not show_images:\n",
    "        return\n",
    "        \n",
    "    shown_images = np.array([[1., 1.]])  # just something big\n",
    "    for i in range(X.shape[0]):\n",
    "        dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "        if np.min(dist) < min_dist: continue\n",
    "        shown_images = np.r_[shown_images, [X[i]]]\n",
    "        imagebox = offsetbox.AnnotationBbox(\n",
    "            offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n",
    "            X[i])\n",
    "        ax.add_artist(imagebox)\n",
    "    plt.xticks([]), plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianRandomProjection\n",
    " * Pick several random axes from normal distribution\n",
    " * Projects data to these axis\n",
    " * Mostly useless for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "Xrp = GaussianRandomProjection(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(Xrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wut?!\n",
    "Not super helpful to say the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(Xrp,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random projections will change if you re-run code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "* Idea: try to compress the data in a way that you can then restore it\n",
    "* Equivalent to minimizing MSE: $|| X  - U \\cdot \\Sigma \\cdot V^T ||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "Xsvd = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(Xsvd[:,:2],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = svd.n_components\n",
    "plt.figure(figsize=[16,8])\n",
    "for i in range(2):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.imshow(svd.components_[i].reshape(8,8),\n",
    "              cmap='gray',interpolation='none')\n",
    "    plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA aka Principial Component Analysis\n",
    "* Idea: we try to find axes that \"explain as much variance as possible\"\n",
    "* Mathematically it's almost the same as SVD\n",
    "* But we'll make you train it anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = <create it>\n",
    "\n",
    "Xpca = <transform data>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(Xpca,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's not compare all 3..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,4])\n",
    "plot_embedding(Xrp,y,ax=plt.subplot(1,3,1),min_dist=3e-2)\n",
    "plot_embedding(Xsvd,y,ax=plt.subplot(1,3,2),min_dist=3e-2)\n",
    "plot_embedding(Xpca,y,ax=plt.subplot(1,3,3),min_dist=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA aka Linear Discriminant Analysis\n",
    "\n",
    "* Idea: let's find axes that allow us to better separate signal from background.\n",
    "* Unlike PCA, this guy actually needs Y data. So you'll have to fit(X,y).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = <create>\n",
    "Xlda = <fit_transform>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(Xlda,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We won!\n",
    "\n",
    "##### Okay, time to make things a bit more complex\n",
    "\n",
    "__Scroll back to the first tab, find the load_digits function and change n_class to 10__\n",
    "\n",
    "Then re-run everything.\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding aka Manifold learning\n",
    "\n",
    "* Instead of finding linear axes, try to just every image with 2D point in a dictionary fashion.\n",
    "\n",
    "* General idea: assign images with 2d points to minimize some structural error. Each algorithm uses it's own error metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional Scaling\n",
    "\n",
    "* Idea: assign similar images into close points and keep different images at distance.\n",
    "\n",
    "#####  More complicated\n",
    "* We try to preserve euclidian distance\n",
    "  * Images can be represented as pixel vectors with euclidian distance defined as\n",
    "$$ r(a,b) = \\sqrt { (a_0 - b_0)^2 + (a_1 - b_1)^2 + ... + (a_63 - b_63)^2}$$\n",
    "  * Our new points are two-dimensional, with 2d euclidian distance\n",
    "$$ r_{new}(a',b') = \\sqrt { (a'_x - b'_x)^2 + (a'_y - b'_y)^2 } $$\n",
    "\n",
    "  * We want $r_{new}$ to be close to $r$\n",
    "  * Move $a'$ and $b'$ to minimize MSE:\n",
    "$$ \\sum _{a,b,a',b'} (r_{new}(a',b') - r(a,b))^2 \\to min_{a',b'} $$\n",
    "\n",
    "  * In code, the mean squared error is denoted as  __stress__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "mds = MDS(n_components=2,verbose=2,n_init=1)\n",
    "Xmds = mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(Xmds,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "t-distributed Stochasitc Neiborhood Embedding\n",
    "\n",
    "Ideologically similar to MDS, but with two differences:\n",
    "* It only cares about close points. The distant points aren't as important.\n",
    "* It uses a different notion of distance (like geodesic distance in graph) formulated through likelihood.\n",
    "\n",
    "\n",
    "In short, it tries to preserve local structure, giving up on global structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tsne = <create>\n",
    "Xtsne = <fit_transform>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_embedding(Xtsne,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning it\n",
    " * t-SNE зависит depends on perplexity ~ how many neighbors to take in account\n",
    "   * See how result changing if you scale perplexity between 1 and 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "<Try 5 different perplexities>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Попробуйте сперва выделить несколько главных компонент с PCA, и только потом применить TSNE\n",
    "   * Для начала, 64D изначально -> 16D после PCA -> 2D после TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<PCA_or_LDA->tsne>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There's a lot more to manifold learning.\n",
    "\n",
    "* Sklearn page - http://scikit-learn.org/stable/modules/manifold.html\n",
    "* Interactive demo - http://distill.pub/2016/misread-tsne/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
